{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d1d4ac-51fd-4ec6-8733-171154f55de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle as pk\n",
    "from glob import glob\n",
    "import os, json, random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime,date, timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from datetime import date\n",
    "import re,ast,random\n",
    "import matplotlib.dates as mdates\n",
    "matplotlib.rcParams['font.size'] = 12\n",
    "matplotlib.rcParams['font.family'] = 'Arial'\n",
    "import seaborn as sns\n",
    "from scipy.stats import mannwhitneyu\n",
    "from collections import Counter \n",
    "import matplotlib\n",
    "import calendar\n",
    "import matplotlib.font_manager as font_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129995bd-71a4-4506-93a6-441ace3ce1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordination metrics:\n",
    "\n",
    "\n",
    "def hashtag_coord(twitter_data,author_id,min_hashes=3):\n",
    "    # default minimum of 3 hashtags; alternatives based on cosine similarity of tweets could work too\n",
    "    # only look at replies or original tweets (not retweets or quotes)\n",
    "    twitter_data2 = twitter_data.loc[(twitter_data['engagementType']=='tweet') | (twitter_data['engagementType']=='reply'),]    # look at text\n",
    "    twitter_text = twitter_data2['contentText'].values.astype(str)\n",
    "    # get hashtag sequence\n",
    "    twitter_data2['hashtag_seq'] = ['__'.join([tag.strip(\"#\").strip('.').strip(',').strip(';').strip('!').strip(':') for tag in tweet.split() if tag.startswith(\"#\")]) for tweet in twitter_text]\n",
    "    # all unique hashtag sequences\n",
    "    unique_hash_seq = twitter_data2['hashtag_seq'].drop_duplicates()\n",
    "    # sort tweets by hashtag sequence\n",
    "    hashes = twitter_data2.groupby('hashtag_seq')\n",
    "    duplicate_hash_users = {}\n",
    "    # for each unique hashtag sequence\n",
    "    for jj,tweet in enumerate(unique_hash_seq):\n",
    "        # ignore if sequence is too short\n",
    "        if len(tweet.split('__')) < min_hashes: continue\n",
    "        # all tweets with this hash sequence\n",
    "        all_tweets = hashes.get_group(tweet)\n",
    "        # all users with this sequence\n",
    "        num_users = len(all_tweets[author_id].drop_duplicates())\n",
    "        # if multiple users\n",
    "        if num_users >1:   \n",
    "            # all unique users\n",
    "            users = all_tweets[author_id].drop_duplicates().tolist()\n",
    "            duplicate_hash_users[tweet] = users\n",
    "    all_dup_hash_users = []\n",
    "    coord_hash_users = []  \n",
    "    # for each hashtag sequence with multiple users\n",
    "    for key in duplicate_hash_users.keys():\n",
    "        # record coord users\n",
    "        coord_hash_users.append(duplicate_hash_users[key])\n",
    "        all_dup_hash_users+=duplicate_hash_users[key]\n",
    "    # network\n",
    "    edges = []\n",
    "    for nodes in coord_hash_users:\n",
    "        unique_edges = list(set([tuple(sorted([n1,n2])) for n1 in nodes for n2 in nodes if n1 != n2]))\n",
    "        edges+=(unique_edges)\n",
    "    # nodes = all user IDs of coorinated users\n",
    "    # edges = if these pairs of users are coordinated\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(edges)\n",
    "    return G\n",
    "\n",
    "\n",
    "def hashtag_coord2(twitter_data,min_hashes=5):\n",
    "    # minimum of 5 hashtags within original tweets; alternatives based on cosine similarity of tweets could work too\n",
    "\n",
    "    twitter_data = twitter_data.loc[(twitter_data['engagementType']=='tweet') | (twitter_data['engagementType']=='reply'),]\n",
    "    #dates = twitter_data['date'].drop_duplicates().sort_values()\n",
    "    tweet_dates = twitter_data['date'].dropna().drop_duplicates().sort_values().values\n",
    "    dates = [[datetime(2022,1,1).date()+timedelta(days=30*i+j) for j in range(30)] for i in range(18)]\n",
    "    \n",
    "    unique_users = twitter_data['twitterAuthorScreenname'].drop_duplicates().values\n",
    "    # keep users with >=5 unique hashtags in a day\n",
    "    daily_twitter_data = twitter_data.groupby('date')\n",
    "    \n",
    "    count = 0\n",
    "    all_edges = set()\n",
    "    for dd,d in enumerate(dates):\n",
    "        print(round(dd/len(dates)*100,2))\n",
    "        user_hashtags = {}\n",
    "        keep_users = set()\n",
    "        day_twitter=pd.concat([daily_twitter_data.get_group(d) for d in dates[dd] if d in tweet_dates])\n",
    "        day_users = day_twitter['twitterAuthorScreenname'].drop_duplicates().values\n",
    "        user_day_twitter = day_twitter.groupby('twitterAuthorScreenname')\n",
    "        for ii,u in enumerate(day_users):#unique_users):\n",
    "            if ii % 10000 == 0:\n",
    "                print('         ',round(ii/len(day_users)*100,2))\n",
    "            ud = user_day_twitter.get_group(u)#user_twitter_data.get_group(u)\n",
    "\n",
    "            \n",
    "            twitter_text = ud['cleanText'].values.astype(str)\n",
    "            #  >= 5 tweets in 7 days\n",
    "            if len(ud) >= min_hashes:\n",
    "                daily_hashes = []\n",
    "                for t in twitter_text:\n",
    "                    daily_hashes += [tag.strip(\"#\") for tag in t.replace('\\n',' ').split() if tag.startswith(\"#\")]\n",
    "                #  >= 5 unique hashtags in 7 days\n",
    "                if len(set(daily_hashes)) >= min_hashes:\n",
    "                    user_hashtags[u] = '__'.join(daily_hashes)\n",
    "                    keep_users.add(u)\n",
    "                    \n",
    "        \n",
    "        edges = set([tuple(sorted([n1,n2])) for n1 in keep_users for n2 in keep_users if n1 != n2 and user_hashtags[n1]==user_hashtags[n2]])\n",
    "        all_edges = all_edges.union(edges)\n",
    "    \n",
    "    # network\n",
    "    # nodes = all user IDs of coorinated users\n",
    "    # edges = if these pairs of users are coordinated\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(all_edges)\n",
    "    return G\n",
    "\n",
    "\n",
    "def retweet_coord(twitter_data,most_similar_cutoff= 0.995):\n",
    "    #most_similar_cutoff = 0.995\n",
    "    # minimum number of retweets\n",
    "    min_retweets=10\n",
    "    rt_doc = []\n",
    "    pred = []\n",
    "    rt_doc = []\n",
    "    num_times = []\n",
    "    unique_users = twitter_data['twitterAuthorScreenname'].drop_duplicates().values\n",
    "    user_twitter_data = twitter_data.groupby('twitterAuthorScreenname')\n",
    "    twitter_data = pd.concat([user_twitter_data.get_group(u).loc[user_twitter_data.get_group(u)['engagementType']=='retweet',] for u in unique_users if len(user_twitter_data.get_group(u).loc[user_twitter_data.get_group(u)['engagementType']=='retweet',])>=min_retweets])\n",
    "    unique_users = twitter_data['twitterAuthorScreenname'].drop_duplicates().values\n",
    "    user_twitter_data = twitter_data.groupby('twitterAuthorScreenname')\n",
    "    # record all retweet IDs\n",
    "    for jj,coord_user in enumerate(unique_users):\n",
    "        if jj % 10000 == 0:\n",
    "            print(round(jj/len(unique_users)*100,2))\n",
    "        coord = user_twitter_data.get_group(coord_user)\n",
    "        tweet_types = coord['engagementType'].drop_duplicates().values\n",
    "        retweets = np.array([])\n",
    "        if 'retweet' in tweet_types:\n",
    "            retweets = coord.loc[coord['engagementType']=='retweet','engagementParentId'].astype(str).values        \n",
    "        rt_doc.append(retweets)\n",
    "        num_times.append(len(retweets))\n",
    "    # save retweet IDs as a long string (so we can use TF-IDF python algorithm)\n",
    "    rt_doc_text = [' '.join(rts.astype(str)) for rts in rt_doc]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(rt_doc_text)\n",
    "    num_times = np.array(num_times)\n",
    "    \n",
    "    user_coord = {}\n",
    "    user_sim = {}    \n",
    "    all_sim = []\n",
    "    for ii,[user,vect] in enumerate(zip(unique_users,X)):\n",
    "        if num_times[ii] < min_retweets: continue\n",
    "        if ii % 1000 == 0:\n",
    "            print(round(ii/len(unique_users)*100,2))            \n",
    "        similarity = cosine_similarity(X[ii],X)\n",
    "        similarity = np.array(similarity)[0]\n",
    "        # remove user ii (self-loop)\n",
    "        # removing clearly dissimilar user pairs (threshold of cosine similarity <= 0.4)\n",
    "        # this action reduces memory load\n",
    "        coord_users = list(unique_users[:])\n",
    "        coord_sim = list(similarity[:])        \n",
    "        coord_num_times = list(num_times[:])\n",
    "        coord_users.pop(ii)\n",
    "        coord_sim.pop(ii)\n",
    "        coord_num_times.pop(ii)\n",
    "        coord_users = np.array(coord_users)\n",
    "        coord_sim = np.array(coord_sim)        \n",
    "        coord_num_times = np.array(coord_num_times) \n",
    "        \n",
    "        user_coord[user] = coord_users[(coord_num_times>=min_retweets)]\n",
    "        user_sim[user] = coord_sim[(coord_num_times>=min_retweets)]\n",
    "        all_sim += user_sim[user].tolist()\n",
    "\n",
    "    min_cosine = np.quantile(all_sim,most_similar_cutoff)\n",
    "    print('MIN COSINE ',min_cosine)\n",
    "    all_weights = []\n",
    "    retweet_edges = []\n",
    "    for user in user_coord.keys():#,time in zip(unique_users,num_times):\n",
    "        user_coord[user] = user_coord[user][user_sim[user] >= min_cosine]\n",
    "        user_sim[user] = user_sim[user][user_sim[user] >= min_cosine]\n",
    "\n",
    "        retweet_edges+=[(user,u) for u in user_coord[user]]#[(user,u) for u,s in zip(user_coord[user],user_sim[user]) if s>min_cosine and u != user]\n",
    "\n",
    "    G2 = nx.Graph()\n",
    "    G2.add_edges_from(retweet_edges)\n",
    "    return min_cosine,G2\n",
    "\n",
    "\n",
    "# bin tweets between min year and max year in 30 minute intervals\n",
    "def time_coord(twitter_data,min_month,min_year,max_month,max_year,most_similar_cutoff=0.995):\n",
    "    min_tweets=10\n",
    "    #most_similar_cutoff = 0.995\n",
    "    bin_size = 30 # 30 minute intervals\n",
    "    first_day,last_day = calendar.monthrange(max_year,max_month)\n",
    "    num_bins = int((datetime(max_year,max_month,last_day)-datetime(min_year,min_month,1)).days*24*60/bin_size)\n",
    "    # bins of 30 minute intervals from min_year to max_year\n",
    "    all_bins = [datetime(min_year,min_month,1)+timedelta(minutes=bin_size*i) for i in range(num_bins)]\n",
    "    all_bins = pd.to_datetime(all_bins)\n",
    "    time_doc = []\n",
    "    num_times = []\n",
    "    unique_users = twitter_data['twitterAuthorScreenname'].drop_duplicates().values\n",
    "    user_twitter_data = twitter_data.groupby('twitterAuthorScreenname')\n",
    "    twitter_data = pd.concat([user_twitter_data.get_group(u) for u in unique_users if len(user_twitter_data.get_group(u))>=min_tweets])\n",
    "    unique_users = twitter_data['twitterAuthorScreenname'].drop_duplicates().values\n",
    "    # saving time tweets are made as strings\n",
    "    # we record whether tweets are made in 30 minute intervals\n",
    "    for jj,coord_user in enumerate(unique_users):\n",
    "        if jj % 50000 == 0:\n",
    "            print(round(jj/len(unique_users)*100,2))        \n",
    "        coord = user_twitter_data.get_group(coord_user)\n",
    "        coord_tweet_time = pd.to_datetime(coord['time_dt'])\n",
    "        # all times where there is 1+ tweets\n",
    "        hist = np.histogram(coord_tweet_time,all_bins)\n",
    "        coord_binned_times = hist[1][:-1][hist[0]>0]        \n",
    "        time_doc.append(coord_binned_times)\n",
    "        num_times.append(len(coord))\n",
    "\n",
    "    time_doc_text = [' '.join(times.astype(str)) for times in time_doc]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(time_doc_text)\n",
    "\n",
    "    num_times = np.array(num_times)\n",
    "    \n",
    "    user_coord = {}\n",
    "    user_sim = {}\n",
    "\n",
    "    all_sim = []\n",
    "    for ii,[user,vect] in enumerate(zip(unique_users,X)):\n",
    "        if num_times[ii] < min_tweets: continue\n",
    "        if ii % 100 == 0:\n",
    "            print(round(ii/len(unique_users)*100,2))            \n",
    "        similarity = cosine_similarity(X[ii],X)\n",
    "        similarity = np.array(similarity)[0]\n",
    "\n",
    "        coord_users = list(unique_users[:])\n",
    "        coord_sim = list(similarity[:])        \n",
    "        coord_num_times = list(num_times[:])\n",
    "        coord_users.pop(ii)\n",
    "        coord_sim.pop(ii)\n",
    "        coord_num_times.pop(ii)\n",
    "        coord_users = np.array(coord_users)\n",
    "        coord_sim = np.array(coord_sim)        \n",
    "        coord_num_times = np.array(coord_num_times) \n",
    "        \n",
    "        user_coord[user] = coord_users[(coord_num_times>=min_tweets)]\n",
    "        user_sim[user] = coord_sim[(coord_num_times>=min_tweets)]\n",
    "        all_sim += user_sim[user].tolist()\n",
    "\n",
    "    min_cosine = np.quantile(all_sim,most_similar_cutoff)\n",
    "    print('MIN COSINE ',min_cosine)\n",
    "    all_weights = []\n",
    "    for user,time in zip(unique_users,num_times):\n",
    "        if user in user_coord.keys() and time >= min_tweets:\n",
    "            user_coord[user] = user_coord[user][user_sim[user] >= min_cosine]\n",
    "            user_sim[user] = user_sim[user][user_sim[user] >= min_cosine]\n",
    "\n",
    "    time_edges = []\n",
    "    for user,time in zip(unique_users,num_times):\n",
    "        if time >= min_tweets:\n",
    "            if user in user_sim.keys():\n",
    "                 time_edges+=[(user,u) for u in user_coord[user]]#[(user,u) for u,s in zip(user_coord[user],user_sim[user]) if s>min_cosine and u != user]\n",
    "\n",
    "    # nodes = coordinated accounts\n",
    "    # edges = which accounts are very similar\n",
    "    G3 = nx.Graph()\n",
    "    G3.add_edges_from(time_edges)\n",
    "    print(len(G3))\n",
    "    G3.remove_edges_from(nx.selfloop_edges(G3))\n",
    "    print(len(G3))\n",
    "    return min_cosine,G3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165ea67-059c-4c9d-8bc5-0049d4c13e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given raw json from API, convert to cleaner and more compact CSV:\n",
    "\n",
    "directory=''\n",
    "twitter_data = []\n",
    "files = list(glob(directory+'control_timeline_2023/*.json'))+list(glob(directory+'control_timeline_12_17/*.json'))\n",
    "files+=list(glob(directory+'any_timeline_1_3/*.json'))+list(glob(directory+'tell_timeline_1_3/*.json'))\n",
    "files = [f for f in files if '_newlines.json' not in f]\n",
    "\n",
    "for ii,file in enumerate(files):\n",
    "    if ii % 30 == 0:\n",
    "        print(round(ii/len(files)*100,2))\n",
    "    new_file = file[:-5]+'_newlines.json'\n",
    "    if not os.path.exists(new_file):\n",
    "        string = []\n",
    "        for l in open(file):\n",
    "            string+=[l.split('}}{\"data')[0]+'}}']+['{\"data'+ll+'}}' for ll in l.split('}}{\"data')[1:-1]] + ['{\"data'+ l.split('}}{\"data')[-1]]\n",
    "        \n",
    "        with open(new_file,'w') as w:\n",
    "            d=json.loads(string[0])\n",
    "            for l in string:\n",
    "                w.write(l+'\\n')# end line = \\n. To open CSVs, we use \"pd.read_csv(...,lineterminator='\\n')\n",
    "\n",
    "files = [file[:-5]+'_newlines.json' for file in files]\n",
    "twitter_data = []\n",
    "kk=2\n",
    "\n",
    "if (kk+1)*500 < len(files):\n",
    "    new_files=files[kk*500:(kk+1)*500]\n",
    "else:\n",
    "    new_files=files[kk*500:]\n",
    "for ii,file in enumerate(new_files):\n",
    "    if ii % 50 == 0:\n",
    "        print([round(ii/len(files)*100,2),file])\n",
    "    try:\n",
    "        twitter_data.append(pd.read_json(file,lines=True))\n",
    "    except:\n",
    "        print(ii)\n",
    "        print(file)\n",
    "twitter_data = pd.concat(twitter_data).reset_index()\n",
    "\n",
    "\n",
    "cols1=set()\n",
    "cols2=set()\n",
    "cols3=set()\n",
    "for ii,[line1,line2] in enumerate(twitter_data[['data','includes']].values):\n",
    "    for tweet in line1:\n",
    "        for key in tweet.keys(): \n",
    "            if type(tweet[key]) == dict:\n",
    "\n",
    "                cols1 = cols1.union(set(list(tweet[key].keys()))) \n",
    "            else:\n",
    "                cols1 = cols1.union(set([key]))           \n",
    "\n",
    "    if 'places' in line2.keys():\n",
    "        for tweet in line2['places']:\n",
    "            for key2 in tweet.keys(): \n",
    "                if type(tweet[key2]) == dict:\n",
    "                    cols2 = cols2.union(set(list(tweet[key2].keys()))) \n",
    "                else:\n",
    "                    cols2 = cols2.union(set([key2])) \n",
    "    if 'users' in line2.keys():\n",
    "        for tweet in line2['users']:\n",
    "            for key2 in tweet.keys(): \n",
    "                if type(tweet[key2]) == dict:\n",
    "                    cols3 = cols3.union(set(list(tweet[key2].keys()))) \n",
    "                else:\n",
    "                    cols3 = cols3.union(set([key2]))                 \n",
    "data1={c:[] for c in cols1}\n",
    "for ii,line1 in enumerate(twitter_data['data'].values):\n",
    "    for tweet in line1:\n",
    "        cols_found = []\n",
    "        for key in tweet.keys(): \n",
    "            if type(tweet[key]) == dict:\n",
    "                for c in tweet[key].keys():\n",
    "                    cols_found.append(c)\n",
    "                    data1[c].append(tweet[key][c])\n",
    "            else:\n",
    "                cols_found.append(key)\n",
    "                data1[key].append(tweet[key])\n",
    "        for c in data1.keys():\n",
    "            if c not in cols_found:\n",
    "                data1[c].append(np.nan)\n",
    "if False:\n",
    "    data2={c:[] for c in cols2}                \n",
    "    for ii,line2 in enumerate(twitter_data['includes'].values):\n",
    "        if 'places' in line2.keys():\n",
    "            for tweet in line2['places']: \n",
    "                cols_found = []\n",
    "                for key2 in tweet.keys(): \n",
    "                    if type(tweet[key2]) == dict:\n",
    "                        for c in tweet[key2].keys():\n",
    "                            cols_found.append(c)\n",
    "                            data2[c].append(tweet[key2][c])\n",
    "                    else:\n",
    "                        cols_found.append(key2)\n",
    "                        data2[key2].append(tweet[key2])\n",
    "                for c in data2.keys():\n",
    "                    if c not in cols_found:\n",
    "                        data2[c].append(np.nan)                    \n",
    "data3={c:[] for c in cols3}                \n",
    "for ii,line2 in enumerate(twitter_data['includes'].values):\n",
    "    if 'users' in line2.keys():\n",
    "        for tweet in line2['users']: \n",
    "            cols_found = []\n",
    "            for key2 in tweet.keys(): \n",
    "                if type(tweet[key2]) == dict:\n",
    "                    for c in tweet[key2].keys():\n",
    "                        cols_found.append(c)\n",
    "                        data3[c].append(tweet[key2][c])\n",
    "                else:\n",
    "                    cols_found.append(key2)\n",
    "                    data3[key2].append(tweet[key2])                \n",
    "            for c in data3.keys():\n",
    "                if c not in cols_found:\n",
    "                    data3[c].append(np.nan)\n",
    "del twitter_data\n",
    "\n",
    "for key in data1.keys():\n",
    "    print([key,data1[key][0]])\n",
    "\n",
    "tweet_types=[]\n",
    "engagementParentIds = []\n",
    "\n",
    "for ii,tweet_refs in enumerate(data1['referenced_tweets']):\n",
    "    if ii % 300000 == 0:\n",
    "        print(round(ii/len(data1['referenced_tweets'])*100,2))\n",
    "    eng_par = np.nan\n",
    "    tweet_type = 'tweet'\n",
    "    if str(tweet_refs) != 'nan':\n",
    "        if len(tweet_refs) == 1:\n",
    "            tweet_refs = tweet_refs[0]\n",
    "            eng_par = tweet_refs['id']\n",
    "            tweet_type = tweet_refs['type']\n",
    "            if tweet_type == 'retweeted':\n",
    "                tweet_type = 'retweet'\n",
    "            elif tweet_type == 'replied_to':\n",
    "                tweet_type = 'reply'\n",
    "            elif tweet_type =='quoted':\n",
    "                tweet_type = 'quote'\n",
    "    engagementParentIds.append(eng_par)\n",
    "    tweet_types.append(tweet_type)\n",
    "data1['engagementParentId'] = engagementParentIds\n",
    "data1['engagementType'] = tweet_types\n",
    "\n",
    "data1 = pd.DataFrame(data1).astype(str).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6bb53-695f-4b66-b741-c66ed4734271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data:\n",
    "\n",
    "new_sample =False\n",
    "if new_sample:\n",
    "    cleaned_data = []\n",
    "    for i in range(3):\n",
    "        init_data = pd.read_csv('cleaned_data_part-'+str(i)+'.csv',lineterminator='\\n')\n",
    "        init_data['date'] = [date.fromtimestamp(t) if str(t) !='nan' else np.nan for t in init_data['time_epoch'].values]\n",
    "        init_data['tweetId'] = init_data['tweetId'].astype(str)\n",
    "        init_data['conversation_id'] = init_data['conversation_id'].astype(str)\n",
    "        init_data['twitterAuthorScreenname'] = init_data['twitterAuthorScreenname'].astype(str)\n",
    "        init_data = init_data.drop_duplicates(subset='tweetId')\n",
    "        cleaned_data.append(init_data)\n",
    "    cleaned_data = pd.concat(cleaned_data)\n",
    "    del init_data\n",
    "    cleaned_data = cleaned_data.drop_duplicates(subset='tweetId')\n",
    "    init_data0 = cleaned_data.loc[cleaned_data['date']<=datetime(2022,12,1).date(),].sample(frac=0.2)\n",
    "    init_data1 = cleaned_data.loc[cleaned_data['date']>datetime(2022,12,1).date(),]    \n",
    "    del cleaned_data\n",
    "    cleaned_data = pd.concat([init_data0,init_data1])\n",
    "    user_cleaned_data = cleaned_data.groupby('twitterAuthorScreenname')\n",
    "    \n",
    "    \n",
    "    cleaned_data.to_csv('equal_sample.csv',index=False)\n",
    "else:\n",
    "    cleaned_data = pd.read_csv('equal_sample.csv',lineterminator='\\n')#'sampled_data_3.csv',lineterminator='\\n')\n",
    "    cleaned_data['twitterAuthorScreenname'] = cleaned_data['twitterAuthorScreenname'].astype(str).values\n",
    "    cleaned_data['date'] = [date.fromtimestamp(t) if str(t) !='nan' else np.nan for t in cleaned_data['time_epoch'].values]\n",
    "    user_cleaned_data = cleaned_data.groupby('twitterAuthorScreenname')\n",
    "    if False:\n",
    "        coord_data_hashtag = []\n",
    "        coord_data_rt = []\n",
    "        coord_data_time = []    \n",
    "        for i in range(3):\n",
    "            part_data = pd.read_csv('cleaned_data_part-'+str(i)+'.csv',lineterminator='\\n')\n",
    "            part_data['twitterAuthorScreenname'] = part_data['twitterAuthorScreenname'].astype(str)\n",
    "            part_users = set(part_data['twitterAuthorScreenname'].drop_duplicates().values.tolist())\n",
    "            user_part_data = part_data.groupby('twitterAuthorScreenname')\n",
    "            coord_data_hashtag.append(pd.concat([user_part_data.get_group(u) for u in hashtag_G.nodes() if u in part_users]))\n",
    "            coord_data_rt.append(pd.concat([user_part_data.get_group(u) for u in retweet_G.nodes() if u in part_users]))\n",
    "            coord_data_time.append(pd.concat([user_part_data.get_group(u) for u in time_G.nodes() if u in part_users]))\n",
    "        coord_data_hashtag = pd.concat(coord_data_hashtag)\n",
    "        coord_data_rt = pd.concat(coord_data_rt)\n",
    "        coord_data_time = pd.concat(coord_data_time)\n",
    "print(len(cleaned_data))\n",
    "cleaned_data = cleaned_data.drop_duplicates(subset='tweetId')\n",
    "print(len(cleaned_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2ce2f-3ce9-45b6-ad72-89421c541205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find coordinated data\n",
    "\n",
    "cutoff=0.999\n",
    "min_hash=3\n",
    "#for min_hash,cutoff in [[3,0.999],[5,0.995],[7,0.9999]]:\n",
    "save_hashtag='hashtag_coord_musk_equal-sample_'+str(min_hash)+'.edgelist'\n",
    "mult=1000\n",
    "if cutoff>0.999:\n",
    "    mult=10000\n",
    "save_time = 'time_coord_musk_equal-sample_'+str(int(cutoff*mult))+'.edgelist'\n",
    "save_rt='retweet_coord_musk_equal-sample_'+str(int(cutoff*mult))+'.edgelist'\n",
    "print(save_hashtag)\n",
    "print(save_time)\n",
    "print(save_rt)\n",
    "\n",
    "save_hashtag='hashtag_coord_musk_equal-sample_'+str(min_hash)+'.edgelist'\n",
    "mult=1000\n",
    "if cutoff>0.999:\n",
    "    mult=10000\n",
    "save_time = 'time_coord_musk_equal-sample_'+str(int(cutoff*mult))+'.edgelist'\n",
    "save_rt='retweet_coord_musk_equal-sample_'+str(int(cutoff*mult))+'.edgelist'\n",
    "\n",
    "hashtag_G = hashtag_coord(cleaned_data,min_hash)\n",
    "\n",
    "nx.write_edgelist(hashtag_G,save_hashtag)    \n",
    "time_thresh,time_G = time_coord(cleaned_data,1,2022,6,2023,cutoff)\n",
    "\n",
    "nx.write_edgelist(time_G,save_time)\n",
    "retweet_thresh,retweet_G = retweet_coord(cleaned_data,cutoff)\n",
    "\n",
    "nx.write_edgelist(retweet_G,save_rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b93b58-79c5-46ef-8c7e-9690f3900d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinated account data:\n",
    "\n",
    "hashtag_nodes = set(list(hashtag_G.nodes()))\n",
    "rt_nodes = set(list(retweet_G.nodes()))\n",
    "time_nodes = set(list(time_G.nodes()))\n",
    "unique_users = set(cleaned_data['twitterAuthorScreenname'].drop_duplicates().values.tolist())\n",
    "twitter_text = cleaned_data['contentText'].values.astype(str)\n",
    "cleaned_data['hashtag_seq'] = ['__'.join([tag.strip(\"#\") for tag in tweet.split() if tag.startswith(\"#\")]) for tweet in twitter_text]\n",
    "unique_hash_seq = cleaned_data['hashtag_seq'].drop_duplicates()\n",
    "user_cleaned_data = cleaned_data.groupby('twitterAuthorScreenname')\n",
    "if not os.path.exists('coord_data_hashtag_min-hash='+str(min_hash)+'.csv'):\n",
    "    coord_data_hashtag = pd.concat([user_cleaned_data.get_group(u) for u in unique_users if u in hashtag_nodes])\n",
    "    user_coord_data_hashtag = coord_data_hashtag.groupby('twitterAuthorScreenname')\n",
    "    coord_data_rt = pd.concat([user_cleaned_data.get_group(u) for u in unique_users if u in rt_nodes])\n",
    "    user_coord_data_rt = coord_data_rt.groupby('twitterAuthorScreenname')\n",
    "    coord_data_time = pd.concat([user_cleaned_data.get_group(u) for u in unique_users if u in time_nodes])\n",
    "    user_coord_data_time = coord_data_time.groupby('twitterAuthorScreenname')\n",
    "\n",
    "\n",
    "    coord_data_hashtag.astype(str).to_csv('coord_data_hashtag_min-hash='+str(min_hash)+'.csv',index=False)\n",
    "    coord_data_rt.astype(str).to_csv('coord_data_rt_cosine_thresh='+str(int(cutoff*mult))+'.csv',index=False)\n",
    "    coord_data_time.astype(str).to_csv('coord_data_time_cosine_thresh='+str(int(cutoff*mult))+'.csv',index=False)\n",
    "else:\n",
    "    coord_data_hashtag=pd.read_csv('coord_data_hashtag_min-hash='+str(min_hash)+'.csv')\n",
    "    coord_data_rt=pd.read_csv('coord_data_rt_cosine_thresh='+str(int(cutoff*mult))+'.csv')\n",
    "    coord_data_time=pd.read_csv('coord_data_time_cosine_thresh='+str(int(cutoff*mult))+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e7f63-00cd-428d-986e-35b22cd14ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between coordinated accounts:\n",
    "\n",
    "jaccard=[]\n",
    "for G2 in [hashtag_G,retweet_G,time_G]:\n",
    "    jaccard.append([])\n",
    "    print(len(G2.nodes()))\n",
    "    for G3 in [hashtag_G,retweet_G,time_G]:\n",
    "        g2nodes= set(list(G2.nodes()))\n",
    "        g3nodes= set(list(G3.nodes()))\n",
    "        intersection = len(g2nodes.intersection(g3nodes))\n",
    "        union=len(g2nodes.union(g3nodes))\n",
    "        jac=intersection/union\n",
    "        jaccard[-1].append(jac)\n",
    "plt.imshow(jaccard)\n",
    "plt.show()\n",
    "for j in jaccard:\n",
    "    print(j)\n",
    "    \n",
    "    \n",
    "jaccard=[]\n",
    "for G2 in [hashtag_G,retweet_G,time_G]:\n",
    "    jaccard.append([])\n",
    "    for G3 in [hashtag_G,retweet_G,time_G]:\n",
    "        g2nodes= set(list(G2.nodes()))\n",
    "        g3nodes= set(list(G3.nodes()))\n",
    "        intersection = len(g2nodes.intersection(g3nodes))\n",
    "        union=len(g2nodes.union(g3nodes))\n",
    "        jac=intersection/len(g2nodes)#union\n",
    "        jaccard[-1].append(jac)\n",
    "plt.imshow(jaccard)\n",
    "plt.show()\n",
    "for j in jaccard:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c94efb-45d6-4665-95a2-db581a6a6010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes in activity data\n",
    "\n",
    "changes_cutoff = []\n",
    "for min_hash,cutoff in [[3,0.999],[3,0.995],[5,0.999],[7,0.9999]]:\n",
    "    save_hashtag='hashtag_coord_musk_equal-sample_'+str(min_hash)+'.edgelist'\n",
    "    mult=1000\n",
    "    if cutoff>0.999:\n",
    "        mult=10000\n",
    "    save_time = 'time_coord_musk_equal-sample_'+str(int(cutoff*mult))+'.edgelist'\n",
    "    save_rt='retweet_coord_musk_equal-sample_'+str(int(cutoff*mult))+'.edgelist'\n",
    "    print(save_hashtag)\n",
    "    print(save_time)\n",
    "    print(save_rt)\n",
    "\n",
    "    save_hashtag='hashtag_coord_musk_equal-sample_'+str(min_hash)+'.edgelist'\n",
    "    mult=1000\n",
    "    if cutoff>0.999:\n",
    "        mult=10000\n",
    "    save_time = 'time_coord_musk_equal-sample_'+str(int(cutoff*mult))+'.edgelist'\n",
    "    save_rt='retweet_coord_musk_equal-sample_'+str(int(cutoff*mult))+'.edgelist'\n",
    "    \n",
    "    hashtag_G = nx.read_edgelist(save_hashtag)\n",
    "    time_G = nx.read_edgelist(save_time)\n",
    "    retweet_G = nx.read_edgelist(save_rt)\n",
    "    changes_type = {}\n",
    "\n",
    "    for ii,G2 in enumerate([hashtag_G,retweet_G,time_G]):\n",
    "        label = ['hashtag','retweet','time'][ii]\n",
    "\n",
    "        nc_nodes = cleaned_data_users - set(list(G2.nodes()))\n",
    "        nc_nodes = list(nc_nodes)[:]\n",
    "        random.shuffle(nc_nodes)\n",
    "        all_dates = cleaned_data['date'].drop_duplicates().values\n",
    "        daily_cleaned_data = cleaned_data.groupby('date')\n",
    "\n",
    "        print(['Hashtag','Retweet','Time'][ii])\n",
    "        print([min_hash,cutoff,len(G2.nodes())])\n",
    "        if len(G2) == 0: continue\n",
    "        nodes = set(list(G2.nodes()))\n",
    "        is_coord = np.array([u in nodes for u in cleaned_data['twitterAuthorScreenname'].values])\n",
    "        coord_data = cleaned_data.loc[is_coord,]\n",
    "        non_coord_data = cleaned_data.loc[~is_coord,]#pd.concat([user_cleaned_data.get_group(u) for u in cleaned_data_users if u not in G2.nodes()])\n",
    "        coord_dates = coord_data['date'].drop_duplicates().values\n",
    "        non_coord_dates = non_coord_data['date'].drop_duplicates().values\n",
    "        daily_coord_data = coord_data.groupby('date')\n",
    "        daily_non_coord_data = non_coord_data.groupby('date') \n",
    "\n",
    "\n",
    "        # these data are:\n",
    "        #.   - daily likes\n",
    "        #.   - daily rts\n",
    "        #.   - daily posts\n",
    "        #.   - mean likes per post each day\n",
    "        #.   - mean rts per post each day\n",
    "        changes = {'num_likes':[[[],[]],[[],[]]],'mean_likes':[[[],[]],[[],[]]],'num_rts':[[[],[]],[[],[]]],'mean_rts':[[[],[]],[[],[]]],'num_posts':[[[],[]],[[],[]]],'num_active':[[[],[]],[[],[]]]}\n",
    "        for i in range(len(dates)):\n",
    "            print(round(i/len(dates),2))\n",
    "            for d in dates[i]:\n",
    "                \n",
    "                if d not in non_coord_dates or d not in coord_dates: continue\n",
    "                non_rt_coord = daily_coord_data.get_group(d).loc[daily_coord_data.get_group(d)['engagementType']!='retweet',]\n",
    "                non_rt_nc = daily_non_coord_data.get_group(d).loc[daily_non_coord_data.get_group(d)['engagementType']!='retweet',]\n",
    "                rt_count_coord = non_rt_coord['retweet_count'].dropna().values\n",
    "                rt_count_nc = non_rt_nc['retweet_count'].dropna().values\n",
    "                like_count_coord = non_rt_coord['like_count'].dropna().values\n",
    "                like_count_nc = non_rt_nc['like_count'].dropna().values\n",
    "                if dates[i][-1]<datetime(2023,6,9).date():\n",
    "\n",
    "                    coord_post_count = len(daily_coord_data.get_group(d))\n",
    "                    nc_post_count = len(daily_non_coord_data.get_group(d))\n",
    "                    coord_user_count = len(daily_coord_data.get_group(d)['twitterAuthorScreenname'].drop_duplicates())\n",
    "                    nc_user_count = len(daily_non_coord_data.get_group(d)['twitterAuthorScreenname'].drop_duplicates())\n",
    "                    if (d < datetime(2022,10,27).date()) and (d > datetime(2022,4,15).date()):\n",
    "                        changes['num_active'][0][0].append(coord_user_count)\n",
    "                        changes['num_active'][1][0].append(nc_user_count)\n",
    "                        changes['num_posts'][0][0].append(coord_post_count)\n",
    "                        changes['num_posts'][1][0].append(nc_post_count)\n",
    "                        changes['num_likes'][0][0].append(like_count_coord.sum())# all likes each day\n",
    "                        changes['num_likes'][1][0].append(like_count_nc.sum())# all likes each day\n",
    "                        changes['num_rts'][0][0].append(rt_count_coord.sum())# all rts each day\n",
    "                        changes['num_rts'][1][0].append(rt_count_nc.sum())# all rts each day\n",
    "\n",
    "                        changes['mean_likes'][0][0].append(like_count_coord.mean())# mean likes per tweet\n",
    "                        changes['mean_likes'][1][0].append(like_count_nc.mean())# mean likes per tweet             \n",
    "                        changes['mean_rts'][0][0].append(rt_count_coord.mean())# mean rts per tweet\n",
    "                        changes['mean_rts'][1][0].append(rt_count_nc.mean())# mean rts per tweet            \n",
    "                    elif d > datetime(2022,10,27).date() and (d < datetime(2022,12,1).date() or d > datetime(2022,12,31).date()):\n",
    "                        changes['num_active'][0][1].append(coord_user_count)\n",
    "                        changes['num_active'][1][1].append(nc_user_count)\n",
    "                        changes['num_posts'][0][1].append(coord_post_count)\n",
    "                        changes['num_posts'][1][1].append(nc_post_count)\n",
    "                        changes['num_likes'][0][1].append(like_count_coord.sum())\n",
    "                        changes['num_likes'][1][1].append(like_count_nc.sum())               \n",
    "                        changes['num_rts'][0][1].append(rt_count_coord.sum())\n",
    "                        changes['num_rts'][1][1].append(rt_count_nc.sum())\n",
    "\n",
    "                        changes['mean_likes'][0][1].append(like_count_coord.mean())\n",
    "                        changes['mean_likes'][1][1].append(like_count_nc.mean())               \n",
    "                        changes['mean_rts'][0][1].append(rt_count_coord.mean())\n",
    "                        changes['mean_rts'][1][1].append(rt_count_nc.mean())             \n",
    "\n",
    "                    #print([[len(changes[key]),len(changes[key][0][0]),len(changes[key][0][1]),len(changes[key][1][0]),len(changes[key][1][1])] for key in changes.keys()])\n",
    "        changes_type[label] = changes\n",
    "    changes_cutoff.append(changes_type)\n",
    "\n",
    "pk.dump(changes_cutoff,open('change_in_posts_robust.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46dad767-ab67-49b6-a4cc-e904c473b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change in activity within coordinated accounts:\n",
    "\n",
    "import scipy\n",
    "cleaned_data_users = set(cleaned_data['twitterAuthorScreenname'].drop_duplicates().values.tolist())\n",
    "#test=np.isin(cleaned_data['twitterAuthorScreenname'].values[:1000000],list(G2.nodes()))\n",
    "new_cutoff = [{'hashtag':changes_cutoff[2]['hashtag'],'retweet':changes_cutoff[1]['retweet'],'time':changes_cutoff[1]['time']}]\n",
    "matplotlib.rcParams['font.size']=16\n",
    "matplotlib.rcParams['font.family']='Arial'\n",
    "for k,cutoff in enumerate(changes_cutoff[:1]+new_cutoff+changes_cutoff[-1:]):\n",
    "    print(['3_999','5_995','7_9999'][k])\n",
    "    min_hash = [3,7,5][k]\n",
    "    cosine_cutoff = [0.999,0.9999,0.995][k]\n",
    "    save_hashtag='hashtag_coord_musk_equal-sample_'+str(min_hash)+'.edgelist'\n",
    "    mult=1000\n",
    "    if cosine_cutoff>0.999:\n",
    "        mult=10000\n",
    "    save_time = 'time_coord_musk_equal-sample_'+str(int(cosine_cutoff*mult))+'.edgelist'\n",
    "    save_rt='retweet_coord_musk_equal-sample_'+str(int(cosine_cutoff*mult))+'.edgelist'\n",
    "    \n",
    "    hashtag_G = nx.read_edgelist(save_hashtag)\n",
    "    time_G = nx.read_edgelist(save_time)\n",
    "    retweet_G = nx.read_edgelist(save_rt)    \n",
    "    for ii,key in enumerate(cutoff['hashtag'].keys()):\n",
    "        print(key)\n",
    "        if key == 'num_active':\n",
    "            fig,ax = plt.subplots(1,1,figsize=(4,4.5))\n",
    "        else:\n",
    "            fig,ax = plt.subplots(1,1,figsize=(4.6,4.05))\n",
    "        \n",
    "        before_after = {}\n",
    "        str_ps = []\n",
    "        means = []\n",
    "        stds = []\n",
    "        for jj,coord_type in enumerate(cutoff.keys()):\n",
    "            G2 = [hashtag_G,retweet_G,time_G][jj]\n",
    "            nc_nodes = cleaned_data_users - set(list(G2.nodes()))\n",
    "\n",
    "            num_coord = len(G2.nodes())\n",
    "\n",
    "            num_non_coord = len(nc_nodes)\n",
    "\n",
    "            before = np.array(cutoff[coord_type][key][0][0])\n",
    "            after = np.array(cutoff[coord_type][key][0][1])\n",
    "            #if coord_type != 'hashtag':# 999 cutoff\n",
    "            #    before = np.array(changes_cutoff[1][coord_type][key][0][0])\n",
    "            #    after = np.array(changes_cutoff[1][coord_type][key][0][1])\n",
    "\n",
    "            before = before[~np.isnan(before)]\n",
    "            after = after[~np.isnan(after)]\n",
    "\n",
    "            coord_mean_before_after = [np.mean(before),np.mean(after)]\n",
    "            print(coord_mean_before_after)\n",
    "            coord_err_before_after = [np.std(before)/np.sqrt(len(before)),np.std(after)/np.sqrt(len(after))]\n",
    "            coord_diff = coord_mean_before_after[1]-coord_mean_before_after[0]\n",
    "            coord_diff_err = np.sqrt(coord_err_before_after[0]**2 + coord_err_before_after[1]**2)\n",
    " \n",
    "            if 'num' in key and key != 'num_posts':\n",
    "                coord_diff /= num_coord\n",
    "                coord_diff_err/=num_coord\n",
    "            else:\n",
    "                print('INIT COORD DIFF: ',coord_diff,' +/- ',coord_diff_err)\n",
    "                coord_diff = 100*(coord_mean_before_after[1]-coord_mean_before_after[0])/coord_mean_before_after[0]\n",
    "                coord_diff_err = 100*np.sqrt(coord_err_before_after[0]**2*(coord_mean_before_after[1]**2/coord_mean_before_after[0]**4) + coord_err_before_after[1]**2/coord_mean_before_after[0]**2)\n",
    "                \n",
    "            if key != 'num_active':\n",
    "                plt.bar([jj],[coord_diff],color=['lightgray','cornflowerblue','greenyellow'][jj],width=0.4)                \n",
    "                plt.errorbar([jj],[coord_diff],yerr=coord_diff_err,color='k',linestyle='')                \n",
    "            else:\n",
    "                plt.bar([jj],[coord_diff],color=['gray','cyan','green'][jj],width=0.4)\n",
    "            coord_r,coord_p = mannwhitneyu(before,after) \n",
    "            \n",
    "            print('Coord P-value: ',round(coord_p,4))\n",
    "            delta = 0.001\n",
    "            if 'mean' in key:\n",
    "                delta = 0.2\n",
    "            if False:\n",
    "                if coord_p < 0.05 and coord_p >= 0.01:\n",
    "                    plt.plot([jj],coord_diff+delta,'k',marker='$*$',markersize=8)\n",
    "                elif coord_p < 0.01 and coord_p >= 0.001:\n",
    "                    plt.plot([jj],coord_diff+delta,'k',marker='$*$$*$',markersize=14)\n",
    "                elif coord_p < 0.001:\n",
    "                    plt.plot([jj],coord_diff+delta,'k',marker='$*$$*$$*$',markersize=20)\n",
    "\n",
    "            before = np.array(cutoff[coord_type][key][1][0])\n",
    "            after = np.array(cutoff[coord_type][key][1][1])\n",
    "            #if coord_type != 'hashtag':# 999 cutoff\n",
    "            #    before = np.array(changes_cutoff[1][coord_type][key][1][0])\n",
    "            #    after = np.array(changes_cutoff[1][coord_type][key][1][1])\n",
    "\n",
    "            before = before[~np.isnan(before)]\n",
    "            after = after[~np.isnan(after)]\n",
    "\n",
    "            nc_mean_before_after = [np.mean(before),np.mean(after)]\n",
    "            \n",
    "            nc_r,nc_p = mannwhitneyu(before,after)    \n",
    "            nc_diff = nc_mean_before_after[1]-nc_mean_before_after[0]\n",
    "            nc_err_before_after = [np.std(before)/np.sqrt(len(before)),np.std(after)/np.sqrt(len(after))]\n",
    "            nc_diff_err = np.sqrt(nc_err_before_after[0]**2 + nc_err_before_after[1]**2)\n",
    "            \n",
    "            if 'num' in key and key != 'num_posts':# and key != 'num_active':\n",
    "                nc_diff /= num_non_coord\n",
    "                nc_diff_err/=num_non_coord\n",
    "            else:\n",
    "                print('INIT DIFF: ',nc_diff,' +/- ',nc_diff_err)\n",
    "                print('change',nc_mean_before_after)\n",
    "                nc_diff = 100*(nc_mean_before_after[1]-nc_mean_before_after[0])/nc_mean_before_after[0]\n",
    "                nc_diff_err = 100*np.sqrt(nc_err_before_after[0]**2*(nc_mean_before_after[1]**2/nc_mean_before_after[0]**4) + nc_err_before_after[1]**2/nc_mean_before_after[0]**2)\n",
    "                print('NC DIFF',nc_diff)\n",
    "            \n",
    "                print('NC P-value: ',round(nc_p,4))                \n",
    "            if key != 'num_active':\n",
    "                plt.bar([jj+0.4],[nc_diff],color=['lightgray','cornflowerblue','greenyellow'][jj],hatch=\"//\",width=0.4)                \n",
    "                plt.errorbar([jj+0.4],[nc_diff],yerr=nc_diff_err,color='k',linestyle='')\n",
    "            else:\n",
    "                plt.bar([jj+0.4],[nc_diff],color=['gray','cyan','green'][jj],hatch=\"o\",width=0.4)\n",
    "            if False:\n",
    "                if nc_p < 0.05 and nc_p > 0.01:\n",
    "                    plt.plot([jj+0.4],nc_diff+delta,'k',marker='$*$',markersize=8)\n",
    "                elif nc_p < 0.01 and nc_p > 0.001:\n",
    "                    plt.plot([jj+0.4],nc_diff+delta,'k',marker='$*$$*$',markersize=14)\n",
    "                elif nc_p < 0.001:\n",
    "                    plt.plot([jj+0.4],nc_diff+delta,'k',marker='$*$$*$$*$',markersize=20)\n",
    "            y_label = key.replace('_',' ').capitalize().replace('rts','Reposts').replace('likes','Likes').replace('Num','Number')\n",
    "            y_label = y_label.replace('post','Post').replace('RePost','Repost')\n",
    "            if 'Number' in y_label: \n",
    "                y_label +=' Per Day Per User'\n",
    "            else:\n",
    "                y_label +=' Per Post'\n",
    "            if key == 'num_posts':\n",
    "                y_label = '% Change in Posts Per Day'                \n",
    "            if key == 'num_active':\n",
    "                y_label = 'Number Active Users Per Day'\n",
    "            z_score = np.abs((coord_diff-nc_diff))/np.sqrt(coord_diff_err**2 + nc_diff_err**2)\n",
    "            \n",
    "            p_values = scipy.stats.norm.sf(abs(z_score))*2\n",
    "            str_p = 'p='+str(round(p_values,2))\n",
    "            if p_values < 0.01:\n",
    "                str_p = 'p='+str(round(p_values,3))\n",
    "            if p_values < 0.001:\n",
    "                str_p = 'p<0.001'\n",
    "            str_ps.append(str_p)\n",
    "            means+=[coord_diff,nc_diff]\n",
    "            stds += [coord_diff_err,nc_diff_err]\n",
    "        if key == 'num_posts':\n",
    "            print(means)\n",
    "            #for jj,coord_type in enumerate(cutoff.keys()):\n",
    "            #    barplot_annotate_brackets(2*jj, 2*jj+1, str_ps[jj], [jj/2 if jj % 2==0 else (jj-1)/2+0.4 for jj in range(len(cutoff.keys())*2)],np.array(means)*1.2,ax=ax)\n",
    "            plt.ylabel(y_label)\n",
    "        if key != 'num_posts':\n",
    "            plt.ylabel('% Change in '+y_label)\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.xticks([0.2,1.2,2.2],['Hashtags','Co-repost','Activity'])\n",
    "        plt.ylim([np.min([np.min(means-stds)*1.1,0]),np.max(means+stds)*1.1])#+0.008\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(key+'_'+['3_999','5_995','7_9999'][k]+'.pdf',transparent=True)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
